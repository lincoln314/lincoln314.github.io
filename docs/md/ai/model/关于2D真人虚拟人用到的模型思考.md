# 图片背景替换算法
    ai及程序上有一些替换模型，但是会出现边缘被切割的感觉。

> 百度飞桨开源,视频背景替换算法：<br/> https://aistudio.baidu.com/aistudio/projectdetail/5690208?forkThirdPart=1

> 😘开源抠图：去除人物背景，且清晰度还不错，边缘不毛躁，已在kaggle上实现。<br/>https://github.com/nadermx/backgroundremover

**需要完善的点**
- 源码中提供的服务端与客户端代码，可以将其部署至linux服务器上，或者用go去覆写（优先级低）
- 打包至行内，与wav2lip融合
- 上述两个算法，将其融合，先抠图，再换背景处理
- 如果要实时，需要用到高并发处理


## 实际需求点
- 方案1：业务会给虚拟人换不同的背景，可以`提前生成`好不同的背景帧。在本地缓存
- 方案2：提前将绿幕形象抠出来，在生成新的视频帧后，再添加背景

😡
**✈✈✈✈ github找材料，很重要的一点是，确定专业名称很关键**
# 视频插针算法
    **Video interpolation**

**github上的项目尝试**
- https://github.com/google-research/frame-interpolation
    >尝试了其中一种，生成效果不太好`可能是因为两张图片之间差别太大了`
    -web版本，上传图片即可生成：
        -https://replicate.com/google-research/frame-interpolation
        -https://huggingface.co/spaces/johngoad/frame-interpolation
- 基于DAIN的改进速度版，中文开源：https://github.com/megvii-research/ECCV2022-RIFE     -- colab已尝试
    - 我的colab运行结果：https://colab.research.google.com/drive/1fPc8850oTcsLZMZHDZ9gFKCEmvb6Pw5Y

## 实际需求点
 >虚拟人动作插帧算法。动作1 + 静默 + 动作2 + 静默，再动作跟静默斜街可能出现跳帧的感官，所以可以提前将动作1最后一帧 （过渡帧）跟静默的第一帧，进行生成保存至动作帧中。进行映射匹配。  -- 工程化完善



# 工程化步骤
- 提前将录制的视频切割成图片，按文件夹命名，需要用时，直接获取
    - 缓存numpy对象(python的缓存库(cacheout))：<br/>https://cloud.tencent.com/developer/article/1115992#:~:text=Python%E7%BC%93%E5%AD%98%E7%A5%9E%E5%A5%87%E5%BA%93cacheout%E5%85%A8%E8%A7%A3%201%20%E5%90%8E%E7%AB%AF%E4%BD%BF%E7%94%A8%E5%AD%97%E5%85%B8%E8%BF%9B%E8%A1%8C%E7%BC%93%E5%AD%98%202%20%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98%E7%AE%A1%E7%90%86%E8%BD%BB%E6%9D%BE%E8%AE%BF%E9%97%AE%E5%A4%9A%E4%B8%AA%E7%BC%93%E5%AD%98%E5%AF%B9%E8%B1%A1%203%20%E5%BD%93%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9D%97%E7%BA%A7%E7%BC%93%E5%AD%98%E5%AF%B9%E8%B1%A1%EF%BC%8C%E9%87%8D%E6%9E%84%E8%BF%90%E8%A1%8C%E6%97%B6%E7%9A%84%E7%BC%93%E5%AD%98%E8%AE%BE%E7%BD%AE%204,LRU%20%28%E6%9C%80%E8%BF%91%E6%9C%80%E5%B0%91%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%88%B6%29%20MRU%20%28%E6%9C%80%E8%BF%91%E6%9C%80%E5%A4%9A%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%88%B6%29%20LFU%20%28%E6%9C%80%E5%B0%8F%E9%A2%91%E7%8E%87%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%88%B6%29%20RR%20%28%E9%9A%8F%E6%9C%BA%E6%9B%BF%E6%8D%A2%E6%9C%BA%E5%88%B6%29
- 语音合成，分片获取结果后，保存为音频文件用于驱动
    - tts引入  分片合成  保存为wav文件
    - 文本跟tts语音的映射关系，根据常规几个字 1s  插入不同动作视频
    - ffmpeg-python处理工具，挺齐全的，看看 <br/>https://github.com/kkroening/ffmpeg-python/blob/master/examples/README.md#jupyter-stream-editor
    - ffmpeg-go <br/>https://github.com/u2takey/ffmpeg-go/tree/d2a537fd00619dc34e668d51626c9660e91bf744
    - 一句话合成，默认静默，如果改子句中有动作则贴动作的帧，取新视频合成
        静默1，动作1，动作2    -->打招呼   ---> 取静默1 + 动作1的图片帧 进行合成即可
- 视频合成后，及时监听推流
    - 合成分段的语音后，进行推流操作  1s为单位生成视频文件，推流出去   
    - 结束帧 怎么生成？
- 添加字幕
    - 机器自动识别语音转换成文字，自动打轴，直接生成字幕，导出带字幕的视频
    - 参考：<br/>https://github.com/wxbool/video-srt
- 静默流的推送
    - 在没有收到下一轮文件调用请求前，进行静默流的推送
    - 接口


- 2s 需要2s少一点 生成时间
    以毫秒为单位进行生成，tts有数据则生成
    实时驱动也可以做
    文本驱动，试一下。接入科大tts


# ai写代码的尝试 copilot
<br/>https://github.com/github-copilot/signup/billing?action=signup_billing&payment_duration=monthly







